{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rgindallas/7-llm-driven-data-engineering/blob/main/Ollama_Setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Ollama in Colab\n",
        "---\n",
        "\n",
        "[![5aharsh/collama](https://raw.githubusercontent.com/5aharsh/collama/main/assets/banner.png)](https://github.com/5aharsh/collama)\n",
        "\n",
        "This is an example notebook which demonstrates how to run Ollama inside a Colab instance. With this you can run pretty much any small to medium sized models offerred by Ollama for free.\n",
        "\n",
        "For the list of available models check [models being offerred by Ollama](https://ollama.com/library).\n",
        "\n",
        "\n",
        "## Before you proceed\n",
        "---\n",
        "\n",
        "Since by default the runtime type of Colab instance is CPU based, in order to use LLM models make sure to change your runtime type to T4 GPU (or better if you're a paid Colab user). This can be done by going to **Runtime > Change runtime type**.\n",
        "\n",
        "While running your script be mindful of the resources you're using. This can be tracked at **Runtime > View resources**.\n",
        "\n",
        "## Running the notebook\n",
        "---\n",
        "\n",
        "After configuring the runtime just run it with **Runtime > Run all**. And you can start tinkering around. This example uses [Llama 3.2](https://ollama.com/library/llama3.2) to generate a response from a prompted question using [LangChain Ollama Integration](https://python.langchain.com/docs/integrations/chat/ollama/)."
      ],
      "metadata": {
        "id": "zyGk-87qnbWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installing Dependencies\n",
        "---\n",
        "\n",
        "1. `pciutils` is required by Ollama to detect the GPU type.\n",
        "2. Installation of Ollama in the runtime instance will be taken care by `curl -fsSL https://ollama.com/install.sh | sh`\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B1S1YL6EnYBB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "id": "YlVK9iG4AD5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Ollama\n",
        "---\n",
        "\n",
        "In order to use Ollama it needs to run as a service in background parallel to your scripts. Becasue Jupyter Notebooks is built to run code blocks in sequence this make it difficult to run two blocks at the same time. As a workaround we will create a service using subprocess in Python so it doesn't block any cell from running.\n",
        "\n",
        "Service can be started by command `ollama serve`.\n",
        "\n",
        "`time.sleep(5)` adds some delay to get the Ollama service up before downloading the model."
      ],
      "metadata": {
        "id": "fGEJwjTPoKWH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)"
      ],
      "metadata": {
        "id": "Jh5CBAFxBYAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pulling Model\n",
        "---\n",
        "\n",
        "Download the LLM model using `ollama pull llama3.2`.\n",
        "\n",
        "For other models check https://ollama.com/library"
      ],
      "metadata": {
        "id": "WcBLqZfyoHg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull llama3.2"
      ],
      "metadata": {
        "id": "o2ghppmRDFny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## And that's it!\n",
        "---\n",
        "\n",
        "With this you should be able to freely play around with the models in your scripts. Following is an example using `langchain-ollama` to answer a simple prompt.\n",
        "\n",
        "If you have a use-case that can help out others feel free to add your notebook to [Collama](https://github.com/5aharsh/collama/fork)"
      ],
      "metadata": {
        "id": "TYQJNeTuni_6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-ollama"
      ],
      "metadata": {
        "id": "MbrT39oil6tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_ollama.llms import OllamaLLM\n",
        "from IPython.display import Markdown\n",
        "\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: Let's think step by step.\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "model = OllamaLLM(model=\"llama3.2\")\n",
        "\n",
        "chain = prompt | model\n",
        "\n",
        "display(Markdown(chain.invoke({\"question\": \"What's the length of hypotenuse in a right angled triangle\"})))"
      ],
      "metadata": {
        "id": "9quBP56zDvpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install openai"
      ],
      "metadata": {
        "id": "x_qFdW4KrEnB",
        "outputId": "1bfdd89e-e3af-4264-c08a-07cfafcec9d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.8.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.11.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Please install OpenAI SDK first: `pip3 install openai`\n",
        "import os\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "#serdata.get('s')\n",
        "client = OpenAI(api_key=userdata.get('DEEPSEEK_API_KEY'), base_url=\"https://api.deepseek.com\")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"deepseek-chat\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hello\"},\n",
        "    ],\n",
        "    stream=False\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "ldG3UV48rpsl",
        "outputId": "626ff06d-55b9-4f5d-ba11-f4c13098d73f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! How can I assist you today? ðŸ˜Š\n"
          ]
        }
      ]
    }
  ]
}